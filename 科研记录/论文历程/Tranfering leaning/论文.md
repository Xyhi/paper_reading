# Transferability in Deep Learning: A Survey

## 1. Introduction

Most mainstream deep learning algorithms requires big datasets to achieve the best performance, yet collecting and annotating such huge amount of data for each task or domain are expensive. 

there are two stages to gain and apply knowledge with good transferability: **pre-training and adaptation.**

> * Pre-training focuses mainly on the **generic** transferability.
> * Adaptation focuses mainly on the **specific** transferability.

### 1.1 Terminology

* Transferability

Given a source domain $\mathcal{S}$ with learning tasks $\mathcal{t}_{\mathcal{S}}$ and a target domain $\mathcal{T}$ with learning tasks $t_{\mathcal{T}}$, transferability is the ability of gaining transferable knowledge from **$t_{\mathcal{S}}$ on $\mathcal{S}$** and reusing the knowledge to decrease **the $\mathcal{generalization}$ error of $t_{\mathcal{T}}$ on $\mathcal{T}$, under the distribution shift $\mathcal{S}\ne \mathcal{T}$ or the task discrepancy $t_{\mathcal{S}}\ne t_{\mathcal{T}}$** 

### 1.2 overview

* **pre-training**

  1. supervised pre-training
  2. unsupervised pre-training

* **adaptation**

  1. task adaptation

     In task adaptation, we first pinpoint several open problems caused by the discrepancy between upstream tasks and downstream tasks, then illustrate how different task adaptation paradigms close the task discrepancy to better utilize the transferability. 

  2. domain adaptation

     we first pinpoint the most influential theories for closing the distribution shift, then elaborate how to derive solid learning algorithms from these theories to enhance the transferability of deep models across domains.

* **evaluation**

domain generalization, out-of-distribution(OOD) generalization, few-shot learning

## 2. pre-training

### 2.1 pre-training

Model architecture has a great influence on the transferability of knowledge obtained via pre-training. The depth of the architecture, or more precisely, the capacity of the model, is deemed the most critical factor to its transferability.

A strong inductive bias makes pre-training of deep models more data-efficient and generalize better when training data is scarce, yet on the other hand, it also limits the expressiveness and transferability of the deep models when there is large-scale data for pre-training.

Transformer removes the local connectivity assumption and models the global dependencies between every two tokens. The connection weights are dynamically
computed by the self-attention mechanism and then the feature aggregation in Transformer depends on these attentions calculated from the input sequence, while the token positions in the sequence are encoded by positional embedding. Transformers are powerful for sequence modeling in natural language processing, and Vision Transformer (ViT) extends them to computer vision.

In summary, Transformer makes least assumptions on the structural information of data, which makes Transformer an expressive architecture for storing the transferable knowledge extracted by pre-training on large amounts of training data.

The success of Transformer reveals that as the amount of pre-training data increases, the learned inductive bias is able to outperform the manually designed inductive bias in terms of transferability.

>简单来说，我们在很多模型中都会加上人为的经验假设，比如我们假定CNN的图像分类kernel只能根据其感受野的区域来进行判别、比如我们假设SVM的损失函数为误分的距离之和等等，但是Transformer解决了这个问题，它是通过自动学习不同的重要性来进行权重的筛选

### 2.2 supervised pre-training

On the other hand, Domain Adaptive Transfer (DAT) (Ngiam et al., 2018) studies the influence of data quality and finds that **using more data does not necessarily lead to better transferability**, especially when the dataset is extremely large. Thus, an importance weighting strategy is proposed to carefully choose the pre-training data that are most relevant to the target task. Cui et al. (2018) also find that pre-training on more similar upstream data improves transferability to fine-grained downstream tasks. 

While standard supervised pre-training is powerful when there are enough labeled data, it still has drawbacks that may limit the transferability of the model. 

standard supervised pre-training models are vulnerable to adversarial examples.

[预训练（pre-training/trained）和微调（fine-tuning） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/389842883)

[CNN入门讲解：什么是微调（Fine Tune）？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/35890660)

#### 2.2.1 Meta-Learning

Meta-learning, also known as learning to learn (Schmidhuber, 1987), aims to pursue such kind of efficient transferability in the pre-training stage.

> * meta-training
>
>   The core idea of meta-learning is to equip the model with some **meta knowledge** $\phi$ that captures intrinsic properties of different learning tasks, which is called meta-training.
>
> * meta-testing
>
>   When facing a new task, the learned meta knowledge could help the target model $\theta$ adapt to the task faster, which is called meta-testing. 

[“元学习”：概念梳理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/289043310)

[《小王爱迁移》系列之二十四：元学习的前世今生 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/146877957)

先来讲一下元学习的概念吧，它是通过两个任务就是$source$和$target$，将我们的数据集切分成$\mathcal{D}_{train}$和$D_test$，但是其主要适用于小样本的学习。目的是将source上的任务迁移到target上。

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221203224005243.png" alt="image-20221203224005243" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221204091503402.png" alt="image-20221204091503402" style="zoom:50%;" />

#### 2.2.2 Causal Learning

[对 Deep InfoMax（DIM）的理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/277660074)

![image-20221204113151521](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221204113151521.png)

![image-20221204113310529](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221204113310529.png)

![image-20221204113443398](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221204113443398.png)

![image-20221204114309759](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221204114309759.png)

![image-20221204114357217](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20221204114357217.png)





# DANN





















